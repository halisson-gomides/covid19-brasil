{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b26be7d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T16:18:11.481714Z",
     "start_time": "2021-05-19T16:18:10.176089Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55cfa987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:08:47.046831Z",
     "start_time": "2021-05-19T20:08:47.038854Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Função de carga de dados\n",
    "\n",
    "def load_data(data_url:str, tipo:str= 'csv', date_f:list= [], **kwargs) -> 'DataFrame':\n",
    "    '''\n",
    "    Função para carregar um conjunto de dados\n",
    "    :rtype: pd.Dataframe\n",
    "    :param data_url: caminho completo do arquivo a ser carregado\n",
    "    :param tipo: tipo de arquivo: csv|xls|xlsx|json etc\n",
    "    :param date_f: lista com os nomes dos campos a ser convertidos para datetime \n",
    "    :param kwargs: argumentos especificos para a carga do arquivo, conforme o parametro 'tipo'\n",
    "    :return: Pandas DataFrame\n",
    "    '''\n",
    "\n",
    "    if tipo == 'csv':\n",
    "        \n",
    "        if 'chunksize' in kwargs:\n",
    "            \n",
    "            kwargs['iterator'] = True\n",
    "            data = pd.DataFrame()\n",
    "            _df_chunked = pd.read_csv(data_url, **kwargs)            \n",
    "            for _df in _df_chunked:\n",
    "                \n",
    "                data = data.append(_df)\n",
    "        else:            \n",
    "            data = pd.read_csv(data_url, **kwargs)\n",
    "            \n",
    "    elif tipo == 'xls' or tipo == 'xlsx':\n",
    "        \n",
    "        data = pd.read_excel(data_url, **kwargs)\n",
    "        \n",
    "    elif tipo == 'json':\n",
    "        \n",
    "        import json\n",
    "        \n",
    "        with open(data_url) as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        if 'chunksize' in kwargs:\n",
    "            \n",
    "            kwargs['iterator'] = True\n",
    "            data = pd.DataFrame()\n",
    "            _df_chunked = pd.read_table(data_url, **kwargs)            \n",
    "            for _df in _df_chunked:\n",
    "                \n",
    "                data = data.append(_df)\n",
    "        else:            \n",
    "            data = pd.read_table(data_url, **kwargs)\n",
    "    \n",
    "    \n",
    "    for dt_field in date_f:\n",
    "        data[dt_field] = pd.to_datetime(data[dt_field])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c516c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T14:31:41.601429Z",
     "start_time": "2021-05-19T14:31:41.594192Z"
    }
   },
   "outputs": [],
   "source": [
    "# Funções de Transformação dos dados\n",
    "\n",
    "async def transform_dfbr(df, cols:list= []) -> 'DataFrame':\n",
    "    '''\n",
    "    Função para transformar o DataFrame df_br, filtra os dados para o Brasil, seleciona colunas especificadas\n",
    "    e adiciona novas colunas\n",
    "    :param df: dataframe com dados sobre a covid do Brasil\n",
    "    :param cols: lista com os nomes das colunas que deverão conter no DataFrame de retorno. Não sendo informado,\n",
    "    retorna todas as colunas do DataFrame.    \n",
    "    :return: Pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    colunas = cols if len(cols) > 0 else df.columns.to_list()\n",
    "    \n",
    "    # Filtra os dados para o Brasil e seleciona colunas específicas\n",
    "    _df_BR = df.query(\"state == 'TOTAL'\")[colunas]\n",
    "\n",
    "    # cria novas colunas\n",
    "    _df_BR['activeCases'] = _df_BR['totalCases'] - _df_BR['deaths'] - _df_BR['recovered']\n",
    "    _df_BR['activeCasesMS'] = _df_BR['totalCasesMS'] - _df_BR['deathsMS'] - _df_BR['recovered']\n",
    "    _df_BR['activeCasesDiff'] = _df_BR['activeCases'] - _df_BR['activeCasesMS']\n",
    "    _df_BR['deathsDiff'] = _df_BR['deaths'] - _df_BR['deathsMS']\n",
    "    _df_BR['newVaccinated'] = _df_BR['vaccinated'].diff()\n",
    "    _df_BR['newVaccinated_second'] = _df_BR['vaccinated_second'].diff()\n",
    "    \n",
    "    return _df_BR\n",
    "\n",
    "\n",
    "async def transform_popuf(df_munic) -> 'DataFrame':\n",
    "    '''\n",
    "    Função para transformar o DataFrame df_popmunic, trata o dado de POPULAÇÃO ESTIMADA e retonar outro dataframe \n",
    "    agregado por UF\n",
    "    :param df_munic: dataframe com dados populacionais dos municípios\n",
    "    :return: Pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    df_munic['POPULAÇÃO ESTIMADA'] = df_munic['POPULAÇÃO ESTIMADA'].apply(lambda x: str(x).split('(')[0])\n",
    "    df_munic['POPULAÇÃO ESTIMADA'] = df_munic['POPULAÇÃO ESTIMADA'].astype(int)\n",
    "    _popuf = df_munic[['UF', 'POPULAÇÃO ESTIMADA']].groupby('UF').sum().reset_index()\n",
    "    \n",
    "    return _popuf\n",
    "\n",
    "\n",
    "async def transform_dfcities(df_cities, df_gps_cities) -> 'DataFrame':\n",
    "    '''\n",
    "    Função para transformar o DataFrame df_cities, acrescentando informações de latitude e longitude a partir do df_gps_cities\n",
    "    :param df_cities: dataframe com dados de covid por município\n",
    "    :param df_gps_cities: dataframe com dados de coordenadas geográficas dos municípios\n",
    "    :return: Pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    # removendo as linhas cujo campo ibgeID está faltando\n",
    "    df_gps_cities = df_gps_cities.dropna(subset=['ibgeID'])\n",
    "    \n",
    "    # convertendo o tipo da coluna ibeID do df_gps_cities para o mesmo tipo da coluna ibgeID do df_cities\n",
    "    df_gps_cities['ibgeID'] = df_gps_cities['ibgeID'].astype(int)\n",
    "    \n",
    "    # definindo as colunas 'lat' e 'lon' no df_cities com base no 'ibgeID' do df_gps_cities\n",
    "    df_cities['lat'] = df_cities['ibgeID'].map(df_gps_cities.set_index('ibgeID')['lat'])\n",
    "    df_cities['lon'] = df_cities['ibgeID'].map(df_gps_cities.set_index('ibgeID')['lon'])\n",
    "    \n",
    "    # filtra pela data mais recente\n",
    "    _df = df_cities.query('date == @df_cities.date.max()')\n",
    "    \n",
    "    return _df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "858ee775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:08:51.505622Z",
     "start_time": "2021-05-19T20:08:51.502641Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Setando variáveis\n",
    "url_br = \"https://raw.githubusercontent.com/wcota/covid19br/master/cases-brazil-states.csv\"\n",
    "url_cities = \"https://github.com/wcota/covid19br/blob/master/cases-brazil-cities-time.csv.gz?raw=true\"\n",
    "url_popmunic = 'datasets/originais/populacao_2020.xls'\n",
    "url_gpscities = \"https://raw.githubusercontent.com/wcota/covid19br/master/gps_cities.csv\"\n",
    "url_geojson_br = 'geojson/brasil-uf-compressed.json'\n",
    "chunk_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cca59e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T16:18:23.003238Z",
     "start_time": "2021-05-19T16:18:22.999249Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "584f207e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T20:12:32.998695Z",
     "start_time": "2021-05-19T20:11:51.850492Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 'gj_br' loaded - 3 rows\n",
      "dataset 'df_popmunic' loaded - 5570 rows\n",
      "dataset 'df_br' loaded - 12138 rows\n",
      "dataset 'df_cities' loaded - 2092289 rows\n",
      "tempo de execução 41.14326110000002\n"
     ]
    }
   ],
   "source": [
    "# Testantdo a função load_data\n",
    "\n",
    "D_ARGS = {\n",
    "    'df_br': dict(data_url=url_br, date_f=['date']),\n",
    "    'df_cities': dict(data_url=url_cities, date_f=['date'], compression='gzip', chunksize=chunk_size),\n",
    "    'df_popmunic': dict(data_url=url_popmunic, tipo='xls', sheet_name='Municípios', skiprows=1, skipfooter=16),\n",
    "    'df_gpscities': dict(data_url=url_gpscities),\n",
    "    'gj_br': dict(data_url=url_geojson_br, tipo='json'),\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    \n",
    "    # Inicia as tarefas de carga e atribui a cada tarefa o nome da chave correspondente a cada conjunto de dados\n",
    "    future_data_loader = {executor.submit(load_data, **valor): chave for chave, valor in D_ARGS.items()}\n",
    "    \n",
    "    for task in as_completed(future_data_loader):\n",
    "        \n",
    "        try:\n",
    "            datasets[future_data_loader[task]] = task.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (future_data_loader[task], exc))\n",
    "        else:\n",
    "            print('dataset %r loaded - %d rows' % (future_data_loader[task], len(datasets[future_data_loader[task]])))\n",
    "\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(\"tempo de execução {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d37662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:03:52.980418Z",
     "start_time": "2021-05-19T15:03:19.148270Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testantdo as funções de transformação\n",
    "start = time.perf_counter()\n",
    "\n",
    "# df_br = await load_data(data_url=url_br, date_f=['date']) # tempo de execução 0.38707780838012695\n",
    "# df_cities = await load_data(data_url=url_cities, date_f=['date'], compression='gzip', iterator=True, chunksize=chunk_size) # tempo de execução 34.51947283744812\n",
    "# df_popmunic = await load_data(data_url=url_popmunic, tipo='xls', sheet_name='Municípios', skiprows=1, skipfooter=16) # tempo de execução 0.37621641159057617\n",
    "\n",
    "df_br, df_cities, df_popmunic = await asyncio.gather(\n",
    "\n",
    ")\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(\"tempo de execução {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f0003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:04:20.897141Z",
     "start_time": "2021-05-19T15:04:20.887139Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2da20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid19-brasil",
   "language": "python",
   "name": "covid19-brasil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
