{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26be7d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T21:58:23.468193Z",
     "start_time": "2021-05-20T21:58:22.710190Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cfa987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T21:58:24.661779Z",
     "start_time": "2021-05-20T21:58:24.653770Z"
    },
    "code_folding": [
     2
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Função de carga de dados\n",
    "\n",
    "def load_data(data_url:str, tipo:str= 'csv', date_f:list= [], **kwargs) -> 'DataFrame':\n",
    "    '''\n",
    "    Função para carregar um conjunto de dados\n",
    "    :rtype: pd.Dataframe\n",
    "    :param data_url: caminho completo do arquivo a ser carregado\n",
    "    :param tipo: tipo de arquivo: csv|xls|xlsx|json etc\n",
    "    :param date_f: lista com os nomes dos campos a ser convertidos para datetime \n",
    "    :param kwargs: argumentos especificos para a carga do arquivo, conforme o parametro 'tipo'\n",
    "    :return: Pandas DataFrame\n",
    "    '''\n",
    "\n",
    "    if tipo == 'csv':\n",
    "        \n",
    "        if 'chunksize' in kwargs:\n",
    "            \n",
    "            kwargs['iterator'] = True\n",
    "            data = pd.DataFrame()\n",
    "            _df_chunked = pd.read_csv(data_url, **kwargs)            \n",
    "            for _df in _df_chunked:\n",
    "                \n",
    "                data = data.append(_df)\n",
    "        else:            \n",
    "            data = pd.read_csv(data_url, **kwargs)\n",
    "            \n",
    "    elif tipo == 'xls' or tipo == 'xlsx':\n",
    "        \n",
    "        data = pd.read_excel(data_url, **kwargs)\n",
    "        \n",
    "    elif tipo == 'json':\n",
    "        \n",
    "        import json\n",
    "        \n",
    "        with open(data_url) as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        if 'chunksize' in kwargs:\n",
    "            \n",
    "            kwargs['iterator'] = True\n",
    "            data = pd.DataFrame()\n",
    "            _df_chunked = pd.read_table(data_url, **kwargs)            \n",
    "            for _df in _df_chunked:\n",
    "                \n",
    "                data = data.append(_df)\n",
    "        else:            \n",
    "            data = pd.read_table(data_url, **kwargs)\n",
    "    \n",
    "    \n",
    "    for dt_field in date_f:\n",
    "        data[dt_field] = pd.to_datetime(data[dt_field])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c516c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T21:58:25.097917Z",
     "start_time": "2021-05-20T21:58:25.086919Z"
    },
    "code_folding": [
     2,
     28,
     43
    ]
   },
   "outputs": [],
   "source": [
    "# Funções de Transformação dos dados\n",
    "\n",
    "async def transform_dfbr(df, cols:list= []) -> 'DataFrame':\n",
    "    '''\n",
    "    Função para transformar o DataFrame df_br, filtra os dados para o Brasil, seleciona colunas especificadas\n",
    "    e adiciona novas colunas\n",
    "    :param df: dataframe com dados sobre a covid do Brasil\n",
    "    :param cols: lista com os nomes das colunas que deverão conter no DataFrame de retorno. Não sendo informado,\n",
    "    retorna todas as colunas do DataFrame.    \n",
    "    :return: Pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    colunas = cols if len(cols) > 0 else df.columns.to_list()\n",
    "    \n",
    "    # Filtra os dados para o Brasil e seleciona colunas específicas\n",
    "    _df_BR = df.query(\"state == 'TOTAL'\")[colunas]\n",
    "\n",
    "    # cria novas colunas\n",
    "    _df_BR['activeCases'] = _df_BR['totalCases'] - _df_BR['deaths'] - _df_BR['recovered']\n",
    "    _df_BR['activeCasesMS'] = _df_BR['totalCasesMS'] - _df_BR['deathsMS'] - _df_BR['recovered']\n",
    "    _df_BR['activeCasesDiff'] = _df_BR['activeCases'] - _df_BR['activeCasesMS']\n",
    "    _df_BR['deathsDiff'] = _df_BR['deaths'] - _df_BR['deathsMS']\n",
    "    _df_BR['newVaccinated'] = _df_BR['vaccinated'].diff()\n",
    "    _df_BR['newVaccinated_second'] = _df_BR['vaccinated_second'].diff()\n",
    "    \n",
    "    return _df_BR\n",
    "\n",
    "\n",
    "async def transform_popuf(df_munic) -> 'DataFrame':\n",
    "    '''\n",
    "    Função para transformar o DataFrame df_popmunic, trata o dado de POPULAÇÃO ESTIMADA e retonar outro dataframe \n",
    "    agregado por UF\n",
    "    :param df_munic: dataframe com dados populacionais dos municípios\n",
    "    :return: Pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    df_munic['POPULAÇÃO ESTIMADA'] = df_munic['POPULAÇÃO ESTIMADA'].apply(lambda x: str(x).split('(')[0])\n",
    "    df_munic['POPULAÇÃO ESTIMADA'] = df_munic['POPULAÇÃO ESTIMADA'].astype(int)\n",
    "    _popuf = df_munic[['UF', 'POPULAÇÃO ESTIMADA']].groupby('UF').sum().reset_index()\n",
    "    \n",
    "    return _popuf\n",
    "\n",
    "\n",
    "async def transform_dfcities(df_cities, df_gps_cities) -> 'DataFrame':\n",
    "    '''\n",
    "    Função para transformar o DataFrame df_cities, acrescentando informações de latitude e longitude a partir do df_gps_cities\n",
    "    :param df_cities: dataframe com dados de covid por município\n",
    "    :param df_gps_cities: dataframe com dados de coordenadas geográficas dos municípios\n",
    "    :return: Pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    # filtra pela data mais recente\n",
    "    _df = df_cities.query('date == @df_cities.date.max()').copy()    \n",
    "    \n",
    "    # removendo as linhas cujo campo ibgeID está faltando\n",
    "    _df_gps = df_gps_cities.dropna(subset=['ibgeID']).copy()\n",
    "    \n",
    "    # convertendo o tipo da coluna ibeID do df_gps_cities para o mesmo tipo da coluna ibgeID do df_cities\n",
    "    _df_gps.loc[:, 'ibgeID'] = _df_gps['ibgeID'].astype(int)\n",
    "    \n",
    "    # definindo as colunas 'lat' e 'lon' no df_cities com base no 'ibgeID' do df_gps_cities\n",
    "    _df['lat'] = _df.loc[:, 'ibgeID'].map(_df_gps.set_index('ibgeID').loc[:, 'lat'])\n",
    "    _df['lon'] = _df.loc[:, 'ibgeID'].map(_df_gps.set_index('ibgeID').loc[:, 'lon'])\n",
    "    \n",
    "    return _df\n",
    "\n",
    "\n",
    "async def transform_dfuf(df, df_popuf) -> 'DataFrame':\n",
    "    '''\n",
    "    Função para transformar o DataFrame df_br, filtra os dados por UF e adiciona nova coluna de percentual \n",
    "    da população vacinada de cada UF com base na informação de população do df_popuf\n",
    "    :param df: dataframe com dados sobre a covid do Brasil\n",
    "    :param df_popuf: dataframe com dados sobre a população estimada por UF\n",
    "    :return: Pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    # Filtra os dados para as UFs e para a data mais recente\n",
    "    _df_UF = df.query(\"state != 'TOTAL' and date == @df['date'].max()\").copy()\n",
    "\n",
    "    # cria nova coluna de percentual de vacinados\n",
    "    _df_UF['perc_vac'] = (_df_UF.loc[:, 'vaccinated'] / _df_UF.loc[:, 'state'].map(df_popuf.set_index('UF').loc[:, 'POPULAÇÃO ESTIMADA']))*100\n",
    "    \n",
    "    return _df_UF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ee775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T21:58:27.771239Z",
     "start_time": "2021-05-20T21:58:27.767276Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Setando variáveis\n",
    "url_br = \"https://raw.githubusercontent.com/wcota/covid19br/master/cases-brazil-states.csv\"\n",
    "url_cities = \"https://github.com/wcota/covid19br/blob/master/cases-brazil-cities-time.csv.gz?raw=true\"\n",
    "url_popmunic = 'datasets/originais/populacao_2020.xls'\n",
    "url_gpscities = \"https://raw.githubusercontent.com/wcota/covid19br/master/gps_cities.csv\"\n",
    "url_geojson_br = 'geojson/brasil-uf-compressed.json'\n",
    "chunk_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca59e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T21:51:47.533742Z",
     "start_time": "2021-05-20T21:51:47.529753Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f207e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:11:34.179146Z",
     "start_time": "2021-05-20T19:11:00.281779Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Testantdo a função load_data\n",
    "\n",
    "D_ARGS = {\n",
    "    'df_br': dict(data_url=url_br, date_f=['date']),\n",
    "    'df_cities': dict(data_url=url_cities, date_f=['date'], compression='gzip', chunksize=chunk_size),\n",
    "    'df_popmunic': dict(data_url=url_popmunic, tipo='xls', sheet_name='Municípios', skiprows=1, skipfooter=16),\n",
    "    'df_gpscities': dict(data_url=url_gpscities),\n",
    "    'gj_br': dict(data_url=url_geojson_br, tipo='json'),\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    \n",
    "    # Inicia as tarefas de carga e atribui a cada tarefa o nome da chave correspondente a cada conjunto de dados\n",
    "    future_data_loader = {executor.submit(load_data, **valor): chave for chave, valor in D_ARGS.items()}\n",
    "    \n",
    "    for task in as_completed(future_data_loader):\n",
    "        \n",
    "        try:\n",
    "            datasets[future_data_loader[task]] = task.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (future_data_loader[task], exc))\n",
    "        else:\n",
    "            print('dataset %r loaded - %d rows' % (future_data_loader[task], len(datasets[future_data_loader[task]])))\n",
    "\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(\"tempo de execução {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d37662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:11:56.778902Z",
     "start_time": "2021-05-20T19:11:56.367958Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Testantdo as funções de transformação\n",
    "start = time.perf_counter()\n",
    "\n",
    "# df_br = await load_data(data_url=url_br, date_f=['date']) # tempo de execução 0.38707780838012695\n",
    "# df_cities = await load_data(data_url=url_cities, date_f=['date'], compression='gzip', iterator=True, chunksize=chunk_size) # tempo de execução 34.51947283744812\n",
    "# df_popmunic = await load_data(data_url=url_popmunic, tipo='xls', sheet_name='Municípios', skiprows=1, skipfooter=16) # tempo de execução 0.37621641159057617\n",
    "\n",
    "df_br, df_cities, df_popuf  = await asyncio.gather(\n",
    "    transform_dfbr(datasets['df_br'], cols=['date', 'state', 'newDeaths', 'deaths', 'deathsMS', 'newCases', 'totalCases', 'totalCasesMS', 'recovered', 'tests', 'vaccinated', 'vaccinated_second']),\n",
    "    transform_dfcities(datasets['df_cities'], datasets['df_gpscities']),\n",
    "    transform_popuf(datasets['df_popmunic']),\n",
    "    \n",
    ")\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(\"tempo de execução {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f0003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T22:05:55.940314Z",
     "start_time": "2021-05-20T22:05:55.930343Z"
    }
   },
   "outputs": [],
   "source": [
    "# Função para retornar os dataframes carregados e transformados\n",
    "\n",
    "async def fetch_dataframes():\n",
    "    \n",
    "    import logging\n",
    "    import time \n",
    "    \n",
    "    # seta o arquivo que armazenará os logs e as configurações básicas de log\n",
    "    logging.basicConfig(filename='gera_graficos_covid.log', \n",
    "                        filemode='w',\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                        datefmt='%d-%m-%Y %H:%M:%S')\n",
    "    \n",
    "    # Instancia o objeto de Log\n",
    "    logger=logging.getLogger() \n",
    "\n",
    "    \n",
    "    # Dicionário cuja chave é o nome do Dataframe a ser carregado e o valor são os parâmetros a serem\n",
    "    # passados para a função que irá carregar o DataFrame - load_data\n",
    "    D_ARGS = {\n",
    "    'df_br': dict(data_url= url_br, date_f=['date']),\n",
    "    'df_cities': dict(data_url= url_cities, date_f=['date'], compression='gzip', chunksize=chunk_size),\n",
    "    'df_popmunic': dict(data_url= url_popmunic, tipo='xls', sheet_name='Municípios', skiprows=1, skipfooter=16),\n",
    "    'df_gpscities': dict(data_url= url_gpscities),\n",
    "    'gj_br': dict(data_url= url_geojson_br, tipo='json'),\n",
    "    }\n",
    "\n",
    "    # Inicializa o Dicionário que irá conter os DataFrames carregados\n",
    "    datasets = {}\n",
    "    \n",
    "    logger.info('Iniciando carga de dados...')\n",
    "    \n",
    "    # Marca o tempo de início da execução da carga\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    # Execução paralela da carga de dados\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "\n",
    "        # Inicia as tarefas de carga e atribui a cada tarefa o nome da chave correspondente a cada conjunto de dados\n",
    "        future_data_loader = {executor.submit(load_data, **valor): chave for chave, valor in D_ARGS.items()}\n",
    "\n",
    "        for task in as_completed(future_data_loader):\n",
    "\n",
    "            try:\n",
    "                datasets[future_data_loader[task]] = task.result()\n",
    "            except Exception as exc:\n",
    "\n",
    "                logger.error(f'{future_data_loader[task]} generated an exception: {exc}') \n",
    "            else:\n",
    "                \n",
    "                logger.info(f'dataset {future_data_loader[task]} carregado - {len(datasets[future_data_loader[task]])} linhas')\n",
    "    \n",
    "    \n",
    "    logger.info(f'Carga de dados finalizada. Tempo de execução: {time.perf_counter() - start} ')\n",
    "    \n",
    "    logger.info('Iniciando tratamento dos dados...')\n",
    "    \n",
    "    # Marca o tempo de início da transformação dos dados\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    df_br, df_cities, df_popuf  = await asyncio.gather(\n",
    "        transform_dfbr(datasets['df_br'], cols=['date', 'state', 'newDeaths', 'deaths', 'deathsMS', 'newCases', 'totalCases', 'totalCasesMS', 'recovered', 'tests', 'vaccinated', 'vaccinated_second']),\n",
    "        transform_dfcities(datasets['df_cities'], datasets['df_gpscities']),\n",
    "        transform_popuf(datasets['df_popmunic']),    \n",
    "    )\n",
    "    df_uf = await transform_dfuf(datasets['df_br'], df_popuf)\n",
    "    \n",
    "    logger.info(f'Tratamento os dados finalizada. Tempo de execução: {time.perf_counter() - start} ')\n",
    "    \n",
    "    return df_br, df_cities, df_popuf, df_uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2da20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T22:06:30.821349Z",
     "start_time": "2021-05-20T22:05:57.048312Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df_br, df_cities, df_popuf, df_uf = await fetch_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8a895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T22:11:20.195899Z",
     "start_time": "2021-05-20T22:11:20.190939Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='gera_graficos_covid.log', \n",
    "                        filemode='w',\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                        datefmt='%d-%m-%Y %H:%M:%S')\n",
    "    \n",
    "# Instancia o objeto de Log\n",
    "logger=logging.getLogger()\n",
    "logger.info('Iniciando carga de dados...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f8693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid19-brasil",
   "language": "python",
   "name": "covid19-brasil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
